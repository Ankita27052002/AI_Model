{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning and Random Forests\n",
    "\n",
    "If you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.\n",
    "\n",
    "For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predictions of all individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Trees is called a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['seaborn-bright','dark_background'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>vintage</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>dependents</th>\n",
       "      <th>occupation</th>\n",
       "      <th>city</th>\n",
       "      <th>customer_nw_category</th>\n",
       "      <th>branch_code</th>\n",
       "      <th>days_since_last_transaction</th>\n",
       "      <th>...</th>\n",
       "      <th>previous_month_end_balance</th>\n",
       "      <th>average_monthly_balance_prevQ</th>\n",
       "      <th>average_monthly_balance_prevQ2</th>\n",
       "      <th>current_month_credit</th>\n",
       "      <th>previous_month_credit</th>\n",
       "      <th>current_month_debit</th>\n",
       "      <th>previous_month_debit</th>\n",
       "      <th>current_month_balance</th>\n",
       "      <th>previous_month_balance</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3135</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>2</td>\n",
       "      <td>755</td>\n",
       "      <td>224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1458.71</td>\n",
       "      <td>1458.71</td>\n",
       "      <td>1449.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1458.71</td>\n",
       "      <td>1458.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2531</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>3</td>\n",
       "      <td>388</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1401.72</td>\n",
       "      <td>1643.31</td>\n",
       "      <td>1871.12</td>\n",
       "      <td>0.33</td>\n",
       "      <td>714.61</td>\n",
       "      <td>588.62</td>\n",
       "      <td>1538.06</td>\n",
       "      <td>1157.15</td>\n",
       "      <td>1677.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>263</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1096.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1666</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16059.34</td>\n",
       "      <td>15211.29</td>\n",
       "      <td>13798.82</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>857.50</td>\n",
       "      <td>286.07</td>\n",
       "      <td>15719.44</td>\n",
       "      <td>15349.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>5922</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7714.19</td>\n",
       "      <td>7859.74</td>\n",
       "      <td>11232.37</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1299.64</td>\n",
       "      <td>439.26</td>\n",
       "      <td>7076.06</td>\n",
       "      <td>7755.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>1145</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>2</td>\n",
       "      <td>317</td>\n",
       "      <td>172.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8519.53</td>\n",
       "      <td>6511.82</td>\n",
       "      <td>16314.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>443.13</td>\n",
       "      <td>5688.44</td>\n",
       "      <td>8563.84</td>\n",
       "      <td>5317.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  vintage  age  gender  dependents  occupation    city  \\\n",
       "0            1     3135   66       0         0.0           0   187.0   \n",
       "1            6     2531   42       0         2.0           0  1494.0   \n",
       "2            7      263   42       1         0.0           0  1096.0   \n",
       "3            8     5922   72       0         0.0           1  1020.0   \n",
       "4            9     1145   46       0         0.0           0   623.0   \n",
       "\n",
       "   customer_nw_category  branch_code  days_since_last_transaction  ...  \\\n",
       "0                     2          755                        224.0  ...   \n",
       "1                     3          388                         58.0  ...   \n",
       "2                     2         1666                         60.0  ...   \n",
       "3                     1            1                         98.0  ...   \n",
       "4                     2          317                        172.0  ...   \n",
       "\n",
       "   previous_month_end_balance  average_monthly_balance_prevQ  \\\n",
       "0                     1458.71                        1458.71   \n",
       "1                     1401.72                        1643.31   \n",
       "2                    16059.34                       15211.29   \n",
       "3                     7714.19                        7859.74   \n",
       "4                     8519.53                        6511.82   \n",
       "\n",
       "   average_monthly_balance_prevQ2  current_month_credit  \\\n",
       "0                         1449.07                  0.20   \n",
       "1                         1871.12                  0.33   \n",
       "2                        13798.82                  0.36   \n",
       "3                        11232.37                  0.64   \n",
       "4                        16314.17                  0.27   \n",
       "\n",
       "   previous_month_credit  current_month_debit  previous_month_debit  \\\n",
       "0                   0.20                 0.20                  0.20   \n",
       "1                 714.61               588.62               1538.06   \n",
       "2                   0.36               857.50                286.07   \n",
       "3                   0.64              1299.64                439.26   \n",
       "4                   0.27               443.13               5688.44   \n",
       "\n",
       "   current_month_balance  previous_month_balance  churn  \n",
       "0                1458.71                 1458.71      0  \n",
       "1                1157.15                 1677.16      1  \n",
       "2               15719.44                15349.75      0  \n",
       "3                7076.06                 7755.98      0  \n",
       "4                8563.84                 5317.04      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('churn_prediction_simple.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = ['churn','customer_id'])\n",
    "Y = data['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17653, 19), (4414, 19), (17653,), (4414,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "x_train, x_test, y_train, y_test = tts(scaled_X, Y, train_size = 0.80,stratify = Y)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "Suppose you have trained a few classifier each one achieving about 80% accuracy. You may have a Logistic Regression Classifier, an SVM classifier,a Random Forest Classifier, a K-Nearest Neighbors classifier and perhaps a few more.\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classfier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n",
    "\n",
    "Voting Classifier often achieves a higher accuracy than the best classifier in the ensemble.\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifier is to train them using very different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ....................... (1 of 3) Processing lr, total=   0.2s\n",
      "[Voting] ....................... (2 of 3) Processing rf, total=  13.6s\n",
      "[Voting] ...................... (3 of 3) Processing svc, total= 1.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 verbose=True, voting='soft')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "        estimators =[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "        voting = 'soft',verbose = True)\n",
    "voting_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VotingClassifier'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8158133212505664\n",
      "RandomForestClassifier 0.8606705935659266\n",
      "SVC 0.8149071137290439\n",
      "[Voting] ....................... (1 of 3) Processing lr, total=   0.2s\n",
      "[Voting] ....................... (2 of 3) Processing rf, total=  13.6s\n",
      "[Voting] ...................... (3 of 3) Processing svc, total= 1.7min\n",
      "VotingClassifier 0.8316719528772089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf,rnd_clf,svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "Another approach to get diverse set of classifier is to use the same training algorithm for every predictor, but to train them on different random subset of the training set.\n",
    "\n",
    "When sampling is done with replacement, this method is called bagging. When sampling is performed without replacement, it is called pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    5.4s remaining:   16.5s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators = 500,\n",
    "            max_samples = 100, n_jobs = -1, bootstrap = True, verbose = 1)\n",
    "\n",
    "bag_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8429995468962392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation\n",
    "\n",
    "63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag instances.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    1.4s remaining:    4.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1, oob_score=True, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators = 500,\n",
    "            max_samples = 100, n_jobs = -1, bootstrap = True, verbose = 1,\n",
    "            oob_score = True)\n",
    "\n",
    "bag_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8440491701127287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed:    0.2s remaining:    0.7s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.842546443135478"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63360324, 0.36639676],\n",
       "       [0.861167  , 0.138833  ],\n",
       "       [0.93522267, 0.06477733],\n",
       "       ...,\n",
       "       [0.83534137, 0.16465863],\n",
       "       [0.71919192, 0.28080808],\n",
       "       [0.85110664, 0.14889336]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1,\n",
       "                       verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "rnd_clf = rfc(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1 , verbose = 1)\n",
    "rnd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8448119619392841"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = rnd_clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = rnd_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "                DecisionTreeClassifier(max_depth = 1), n_estimators = 100,\n",
    "                algorithm = 'SAMME.R', learning_rate = 0.5)\n",
    "\n",
    "ada_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8509288627095605"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = ada_clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "It works by sequentially adding predictors to an ensemble, each one correcting it's predecessor. But instead of tweaking the instances weigths at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y_train - tree_reg1.predict(x_train)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(x_train, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(x_train)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(x_train, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the tress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(x_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the optimal number of trees you can use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=119)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(x_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbrt.staged_predict(x_test)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
